
@article{nalborczyk_orofacial_2017,
	title = {Orofacial electromyographic correlates of induced verbal rumination},
	volume = {127},
	issn = {03010511},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0301051117300820},
	doi = {10.1016/j.biopsycho.2017.04.013},
	language = {en},
	urldate = {2018-02-09},
	journal = {Biological Psychology},
	author = {Nalborczyk, Ladislas and Perrone-Bertolotti, M and Baeyens, Céline and Grandchamp, Romain and Polosan, Mircea and Spinelli, Elsa and Koster, Ernst H.W. and Lœvenbruck, Hélène},
	year = {2017},
	pages = {53--63},
	file = {Nalborczyk et al. - 2017 - Orofacial electromyographic correlates of induced .pdf:/Users/Ladislas/Zotero/storage/QPRTPQQV/Nalborczyk et al. - 2017 - Orofacial electromyographic correlates of induced .pdf:application/pdf;Nalborczyk et al. - 2017 - Orofacial electromyographic correlates of induced .pdf:/Users/Ladislas/Zotero/storage/F3Y6RFC9/Nalborczyk et al. - 2017 - Orofacial electromyographic correlates of induced .pdf:application/pdf;Nalborczyk et al. - 2017.pdf:/Users/Ladislas/Zotero/storage/GXYWHNYN/Nalborczyk et al. - 2017.pdf:application/pdf}
}

@article{guillot_imagining_2012,
	title = {Imagining is not doing but involves specific motor commands: {A} review of experimental data related to motor inhibition},
	volume = {6},
	issn = {1662-5161},
	shorttitle = {Imagining is {Not} {Doing} but {Involves} {Specific} {Motor} {Commands}},
	url = {http://journal.frontiersin.org/article/10.3389/fnhum.2012.00247/abstract},
	doi = {10.3389/fnhum.2012.00247},
	urldate = {2018-03-08},
	journal = {Frontiers in Human Neuroscience},
	author = {Guillot, Aymeric and Di Rienzo, Franck and MacIntyre, Tadhg and Moran, Aidan and Collet, Christian},
	year = {2012},
	file = {Guillot et al. - 2012 - Imagining is Not Doing but Involves Specific Motor.pdf:/Users/Ladislas/Zotero/storage/GU2AEFBG/Guillot et al. - 2012 - Imagining is Not Doing but Involves Specific Motor.pdf:application/pdf;Guillot et al. - 2012 - Imagining is Not Doing but Involves Specific Motor.pdf:/Users/Ladislas/Zotero/storage/RSSFUB2N/Guillot et al. - 2012 - Imagining is Not Doing but Involves Specific Motor.pdf:application/pdf;Guillot et al. 2012 Front Hum Neurosci.pdf:/Users/Ladislas/Zotero/storage/PWLYRYJV/Guillot et al. 2012 Front Hum Neurosci.pdf:application/pdf}
}

@article{alderson-day_inner_2015,
	title = {Inner speech: {Development}, cognitive functions, phenomenology, and neurobiology.},
	volume = {141},
	issn = {1939-1455, 0033-2909},
	shorttitle = {Inner speech},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/bul0000021},
	doi = {10.1037/bul0000021},
	language = {en},
	number = {5},
	urldate = {2018-03-29},
	journal = {Psychological Bulletin},
	author = {Alderson-Day, Ben and Fernyhough, Charles},
	month = sep,
	year = {2015},
	keywords = {auditory verbal hallucinations, covert speech, developmental disorders, private speech},
	pages = {931--965},
	file = {Alderson-Day & Fernyhough - 2015.pdf:/Users/Ladislas/Zotero/storage/CM57H5DE/Alderson-Day & Fernyhough - 2015.pdf:application/pdf;Alderson-Day & Fernyhough - 2015.pdf:/Users/Ladislas/Zotero/storage/KM3DM6YK/Alderson-Day & Fernyhough - 2015.pdf:application/pdf;Alderson-Day, Fernyhough - 2015a.pdf:/Users/Ladislas/Zotero/storage/2JL4LKFJ/Alderson-Day, Fernyhough - 2015a.pdf:application/pdf;Alderson-Day, Fernyhough - 2015a.pdf:/Users/Ladislas/Zotero/storage/BJZHKA8I/Alderson-Day, Fernyhough - 2015a.pdf:application/pdf;Alderson-Day, Fernyhough - 2015a.pdf:/Users/Ladislas/Desktop/Articles/Alderson-Day, Fernyhough - 2015a.pdf:application/pdf}
}

@article{garrity_electromyography:_1977,
	title = {Electromyography: {A} review of the current status of subvocal speech research},
	volume = {5},
	issn = {0090-502X, 1532-5946},
	shorttitle = {Electromyography},
	url = {http://www.springerlink.com/index/10.3758/BF03197407},
	doi = {10.3758/BF03197407},
	language = {en},
	number = {6},
	urldate = {2018-03-30},
	journal = {Memory \& Cognition},
	author = {Garrity, Linda I.},
	month = nov,
	year = {1977},
	pages = {615--622},
	file = {Garrity - 1977 - Electromyography A review of the current status o.pdf:/Users/Ladislas/Zotero/storage/VUV48NYQ/Garrity - 1977 - Electromyography A review of the current status o.pdf:application/pdf;Garrity - 1977 - Electromyography A review of the current status o.pdf:/Users/Ladislas/Zotero/storage/3KZFVPC8/Garrity - 1977 - Electromyography A review of the current status o.pdf:application/pdf}
}

@article{lapatki_optimal_2010,
	title = {Optimal placement of bipolar surface {EMG} electrodes in the face based on single motor unit analysis},
	volume = {47},
	issn = {00485772, 14698986},
	url = {http://doi.wiley.com/10.1111/j.1469-8986.2009.00935.x},
	doi = {10.1111/j.1469-8986.2009.00935.x},
	abstract = {Locations of surface electromyography (sEMG) electrodes in the face are usually chosen on a macro-anatomical basis. In this study we describe optimal placement of bipolar electrodes based on a novel method and present results for lower facial muscles. We performed high-density sEMG recordings in 13 healthy participants. Raw sEMG signals were decomposed into motor unit action potentials (MUAPs). We positioned virtual electrode pairs in the interpolated monopolar MUAPs at different positions along muscle ﬁber direction and calculated the bipolar potentials. Electrode sites were determined where maximal bipolar amplitude was achieved and were validated. Objective guidelines for sEMG electrode placement improve the signal-to-noise ratio and may contribute to reduce cross talk, which is particularly important in the face. The method may be regarded as an important basis for improving the validity and reproducibility of sEMG in complex muscle areas.},
	language = {en},
	number = {2},
	urldate = {2018-03-30},
	journal = {Psychophysiology},
	author = {Lapatki, Bernd G. and Oostenveld, Robert and Van Dijk, Johannes P. and Jonas, Irmtrud E. and Zwarts, Machiel J. and Stegeman, Dick F.},
	month = mar,
	year = {2010},
	pages = {299--314},
	file = {Lapatki et al. - 2010 - Optimal placement of bipolar surface EMG electrode.pdf:/Users/Ladislas/Zotero/storage/KNSU6PA3/Lapatki et al. - 2010 - Optimal placement of bipolar surface EMG electrode.pdf:application/pdf}
}

@article{Rapin2013,
	title = {An {EMG} study of the lip muscles during covert auditory verbal hallucinations in schizophrenia},
	volume = {56},
	issn = {1558-9102},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/24687444},
	doi = {10.1044/1092-4388(2013/12-0210)},
	abstract = {PURPOSE: Auditory verbal hallucinations (AVHs) are speech perceptions in the absence of external stimulation. According to an influential theoretical account of AVHs in schizophrenia, a deficit in inner-speech monitoring may cause the patients' verbal thoughts to be perceived as external voices. The account is based on a predictive control model, in which individuals implement verbal self-monitoring. The authors examined lip muscle activity during AVHs in patients with schizophrenia to check whether inner speech occurred.

METHOD: Lip muscle activity was recorded during covert AVHs (without articulation) and rest. Surface electromyography (EMG) was used on 11 patients with schizophrenia.

RESULTS: Results showed an increase in EMG activity in the orbicularis oris inferior muscle during covert AVHs relative to rest. This increase was not due to general muscular tension because there was no increase of muscular activity in the forearm muscle.

CONCLUSION: This evidence that AVHs might be self-generated inner speech is discussed in the framework of a predictive control model. Further work is needed to better describe how inner speech is controlled and monitored and the nature of inner-speech-monitoring-dysfunction. This will lead to a better understanding of how AVHs occur.},
	number = {6},
	journal = {Journal of speech, language, and hearing research},
	author = {Rapin, Lucile and Dohen, Marion and Polosan, Mircea and Perrier, Pascal and Lœvenbruck, Hélène},
	year = {2013},
	pmid = {24687444},
	keywords = {inner speech, auditory verbal hallucinations, emg, internal models, orbicularis oris, self-monitoring deficit},
	pages = {1882--1993},
	annote = {Extracted Annotations
chin = menton(note on p. 3)},
	file = {Rapin et al. - 2013 - An EMG Study of the Lip Muscles During Covert Audi.pdf:/Users/Ladislas/Zotero/storage/E2YT7SB6/Rapin et al. - 2013 - An EMG Study of the Lip Muscles During Covert Audi.pdf:application/pdf;Rapin et al. - 2013 - An EMG Study of the Lip Muscles During Covert Audi.pdf:/Users/Ladislas/Zotero/storage/DSKP7NTI/Rapin et al. - 2013 - An EMG Study of the Lip Muscles During Covert Audi.pdf:application/pdf;Rapin et al. - 2013 - An EMG study of the lip muscles during covert auditory verbal hallucinations in schizophrenia.pdf:/Users/Ladislas/Desktop/Articles/Rapin et al. - 2013 - An EMG study of the lip muscles during covert auditory verbal hallucinations in schizophrenia.pdf:application/pdf;Rapin et al. - 2014 - An EMG Study of the Lip Muscles During Covert Auditory Verbal Hallucinations in Schizophrenia.pdf:/Users/Ladislas/Zotero/storage/G6EBKNBP/Rapin et al. - 2014 - An EMG Study of the Lip Muscles During Covert Auditory Verbal Hallucinations in Schizophrenia.pdf:application/pdf}
}

@article{mcguigan_discriminative_1974,
	title = {Discriminative relationship between covert oral behavior and the phonemic system in internal information processing.},
	volume = {103},
	issn = {0022-1015},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/h0037379},
	doi = {10.1037/h0037379},
	language = {en},
	number = {5},
	urldate = {2018-09-04},
	journal = {Journal of Experimental Psychology},
	author = {McGuigan, F. J. and Winstead, C. L.},
	year = {1974},
	pages = {885--890},
	file = {McGuigan and Winstead - 1974 - Discriminative relationship between covert oral be.pdf:/Users/Ladislas/Zotero/storage/6B8JTQG8/McGuigan and Winstead - 1974 - Discriminative relationship between covert oral be.pdf:application/pdf;McGuigan and Winstead - 1974 - Discriminative relationship between covert oral be.pdf:/Users/Ladislas/Zotero/storage/8SC6P8DZ/McGuigan and Winstead - 1974 - Discriminative relationship between covert oral be.pdf:application/pdf}
}

@article{mcguigan_patterns_1989,
	title = {Patterns of covert speech behavior and phonetic coding},
	volume = {24},
	abstract = {In previous research a discriminative relationship has been established between patterns of covert speech behavior and the phonemic system when processing continuous linguistic material. The goal of the present research was to be more analytic and pinpoint covert neuromuscular speech patterns when one processes specific instances of phonemes. Electromyographic (EMG) recording indicated that the lips are significantly active when visually processing the letter "P" (an instance of bilabial material), but not when processing the letter "T" or a nonlinguistic control (C) stimulus. Similarly, the tongue is significantly active when processing the letter "T" (an instance of lingual-alveolar material), but not when processing the letters "P" or "C." It is concluded that the speech musculature covertly responds systematically as a function of class of phoneme being processed. These results accord with our model that semantic processing ("understanding") occurs when the speech (and other) musculature interacts with linguistic regions of the brain. In the interactions phonetic coding is generated and transmitted through neuromuscular circuits that have cybernetic characteristics.},
	language = {en},
	number = {1},
	journal = {The Pavlovian Journal of Biological Science},
	author = {McGuigan, F. J. and Dollins, A. D.},
	year = {1989},
	pages = {19--26},
	file = {McGUIGAN - Patterns of covert speech behavior and phonetic co.pdf:/Users/Ladislas/Zotero/storage/X6DJ8QK8/McGUIGAN - Patterns of covert speech behavior and phonetic co.pdf:application/pdf}
}

@incollection{loevenbruck_cognitive_2018,
	title = {A cognitive neuroscience view of inner language: to predict and to hear, see, feel},
	language = {en},
	booktitle = {Inner speech: {New} voices},
	publisher = {Oxford University Press},
	author = {Lœvenbruck, Hélène and Grandchamp, R and Rapin, Lucile and Nalborczyk, Ladislas and Dohen, M and Perrier, P and Baciu, M and Perrone-Bertolotti, M},
	editor = {Langland-Hassan, Peter and Vicente, Agustín},
	year = {2018},
	pages = {37},
	file = {Lœvenbruck et al. - A Cognitive Neuroscience View of Inner Language.pdf:/Users/Ladislas/Zotero/storage/B887UFJJ/Lœvenbruck et al. - A Cognitive Neuroscience View of Inner Language.pdf:application/pdf}
}

@article{locke_subvocal_1970,
	title = {Subvocal rehearsal as a form of speech},
	volume = {9},
	doi = {10.1016/s0022-5371(70)80092-5},
	number = {5},
	journal = {Journal of Verbal Learning and Verbal Behavior},
	author = {Locke, J. L. and Fehr, F. S.},
	year = {1970},
	pages = {495--498},
	file = {Locke & Fehr - 1970.pdf:/Users/Ladislas/Zotero/storage/C5I38AZH/Locke & Fehr - 1970.pdf:application/pdf}
}

@article{locke_subvocal_1970-1,
	title = {Subvocal speech and speech},
	volume = {12},
	journal = {ASHA},
	author = {Locke, J. L.},
	year = {1970},
	pages = {7--14},
	file = {Locke_1970.pdf:/Users/Ladislas/Zotero/storage/DT756LKM/Locke_1970.pdf:application/pdf}
}

@article{eskes_predicting_2017,
	title = {Predicting {3D} lip shapes using facial surface {EMG}},
	volume = {12},
	issn = {1932-6203},
	doi = {10.1371/journal.pone.0175025},
	language = {en},
	number = {4},
	urldate = {2018-12-04},
	journal = {PLOS ONE},
	author = {Eskes, Merijn and van Alphen, Maarten J. A. and Balm, Alfons J. M. and Smeele, Ludi E. and Brandsma, Dieta and van der Heijden, Ferdinand},
	editor = {Zhang, Yingchun},
	year = {2017},
	pages = {e0175025},
	file = {Eskes et al. - 2017 - Predicting 3D lip shapes using facial surface EMG.pdf:/Users/Ladislas/Zotero/storage/6X43GW3K/Eskes et al. - 2017 - Predicting 3D lip shapes using facial surface EMG.pdf:application/pdf}
}

@inproceedings{kapur_alterego:_2018,
	address = {Tokyo, Japan},
	title = {{AlterEgo}: {A} personalized wearable silent speech interface},
	isbn = {978-1-4503-4945-1},
	shorttitle = {{AlterEgo}},
	doi = {10.1145/3172944.3172977},
	abstract = {We present a wearable interface that allows a user to silently converse with a computing device without any voice or any discernible movements - thereby enabling the user to communicate with devices, AI assistants, applications or other people in a silent, concealed and seamless manner. A user's intention to speak and internal speech is characterized by neuromuscular signals in internal speech articulators that are captured by the AlterEgo system to reconstruct this speech. We use this to facilitate a natural language user interface, where users can silently communicate in natural language and receive aural output (e.g - bone conduction headphones), thereby enabling a discreet, bi-directional interface with a computing device, and providing a seamless form of intelligence augmentation. The paper describes the architecture, design, implementation and operation of the entire system. We demonstrate robustness of the system through user studies and report 92\% median word accuracy levels.},
	language = {en},
	urldate = {2019-01-11},
	booktitle = {Proceedings of the 2018 {Conference} on {Human} {Information} {Interaction}\&{Retrieval}  - {IUI} '18},
	publisher = {ACM Press},
	author = {Kapur, Arnav and Kapur, Shreyas and Maes, Pattie},
	year = {2018},
	pages = {43--53},
	file = {Kapur et al. - 2018 - AlterEgo A Personalized Wearable Silent Speech In.pdf:/Users/Ladislas/Zotero/storage/K4XUCBCD/Kapur et al. - 2018 - AlterEgo A Personalized Wearable Silent Speech In.pdf:application/pdf}
}

@article{guillot_understanding_2012,
	title = {Understanding the timing of motor imagery: recent findings and future directions},
	volume = {5},
	issn = {1750-984X, 1750-9858},
	shorttitle = {Understanding the timing of motor imagery},
	url = {http://www.tandfonline.com/doi/abs/10.1080/1750984X.2011.623787},
	doi = {10.1080/1750984X.2011.623787},
	language = {en},
	number = {1},
	urldate = {2019-02-13},
	journal = {International Review of Sport and Exercise Psychology},
	author = {Guillot, Aymeric and Hoyek, Nady and Louis, Magali and Collet, Christian},
	month = mar,
	year = {2012},
	pages = {3--22},
	file = {Guillot et al. - 2012 - Understanding the timing of motor imagery recent .pdf:/Users/Ladislas/Zotero/storage/52H9W5ZY/Guillot et al. - 2012 - Understanding the timing of motor imagery recent .pdf:application/pdf}
}

@article{martin_decoding_2014,
	title = {Decoding spectrotemporal features of overt and covert speech from the human cortex},
	volume = {7},
	issn = {1662-6443},
	url = {http://journal.frontiersin.org/article/10.3389/fneng.2014.00014/abstract},
	doi = {10.3389/fneng.2014.00014},
	abstract = {Auditory perception and auditory imagery have been shown to activate overlapping brain regions. We hypothesized that these phenomena also share a common underlying neural representation. To assess this, we used electrocorticography intracranial recordings from epileptic patients performing an out loud or a silent reading task. In these tasks, short stories scrolled across a video screen in two conditions: subjects read the same stories both aloud (overt) and silently (covert). In a control condition the subject remained in a resting state. We ﬁrst built a high gamma (70–150 Hz) neural decoding model to reconstruct spectrotemporal auditory features of self-generated overt speech. We then evaluated whether this same model could reconstruct auditory speech features in the covert speech condition. Two speech models were tested: a spectrogram and a modulation-based feature space. For the overt condition, reconstruction accuracy was evaluated as the correlation between original and predicted speech features, and was signiﬁcant in each subject (p {\textless} 10−5; paired two-sample t-test). For the covert speech condition, dynamic time warping was ﬁrst used to realign the covert speech reconstruction with the corresponding original speech from the overt condition. Reconstruction accuracy was then evaluated as the correlation between original and reconstructed speech features. Covert reconstruction accuracy was compared to the accuracy obtained from reconstructions in the baseline control condition. Reconstruction accuracy for the covert condition was signiﬁcantly better than for the control condition (p {\textless} 0.005; paired two-sample t-test). The superior temporal gyrus, pre- and post-central gyrus provided the highest reconstruction information. The relationship between overt and covert speech reconstruction depended on anatomy. These results provide evidence that auditory representations of covert speech can be reconstructed from models that are built from an overt speech data set, supporting a partially shared neural substrate.},
	language = {en},
	urldate = {2019-02-13},
	journal = {Frontiers in Neuroengineering},
	author = {Martin, StÃ©phanie and Brunner, Peter and Holdgraf, Chris and Heinze, Hans-Jochen and Crone, Nathan E. and Rieger, Jochem and Schalk, Gerwin and Knight, Robert T. and Pasley, Brian N.},
	month = may,
	year = {2014},
	file = {Martin et al. - 2014 - Decoding spectrotemporal features of overt and cov.pdf:/Users/Ladislas/Zotero/storage/GMZWJVQJ/Martin et al. - 2014 - Decoding spectrotemporal features of overt and cov.pdf:application/pdf}
}

@article{martin_word_2016,
	title = {Word pair classification during imagined speech using direct brain recordings},
	volume = {6},
	issn = {2045-2322},
	url = {http://www.nature.com/articles/srep25803},
	doi = {10.1038/srep25803},
	language = {en},
	number = {1},
	urldate = {2019-02-13},
	journal = {Scientific Reports},
	author = {Martin, Stephanie and Brunner, Peter and Iturrate, Iñaki and Millán, José del R. and Schalk, Gerwin and Knight, Robert T. and Pasley, Brian N.},
	month = sep,
	year = {2016},
	file = {Martin et al. - 2016 - Word pair classification during imagined speech us.pdf:/Users/Ladislas/Zotero/storage/34J3ZG5H/Martin et al. - 2016 - Word pair classification during imagined speech us.pdf:application/pdf}
}

@article{mcguigan_effects_1968,
	title = {Effects of auditory stimulation on covert oral behavior during silent reading},
	volume = {76},
	issn = {0022-1015},
	doi = {10.1037/h0025673},
	language = {eng},
	number = {4},
	journal = {Journal of Experimental Psychology},
	author = {McGuigan, F. J. and Rodier, W. I.},
	month = apr,
	year = {1968},
	pmid = {5650583},
	keywords = {Humans, Female, Electromyography, Rest, Reading, Auditory Perception, Respiration, Arm, Chin, Noise, Sound, Tongue},
	pages = {649--655}
}

@article{martin_decoding_2018,
	title = {Decoding inner speech using electrocorticography: {Progress} and challenges toward a speech prosthesis},
	volume = {12},
	issn = {1662-453X},
	shorttitle = {Decoding {Inner} {Speech} {Using} {Electrocorticography}},
	url = {https://www.frontiersin.org/articles/10.3389/fnins.2018.00422/full},
	doi = {10.3389/fnins.2018.00422},
	abstract = {Certain brain disorders resulting from brainstem infarcts, traumatic brain injury, cerebral palsy, stroke and amyotrophic lateral sclerosis, limit verbal communication despite the patient being fully aware. People that cannot communicate due to neurological disorders would benefit from a system that can infer internal speech directly from brain signals. In this review article, we describe the state of the art in decoding inner speech, ranging from early acoustic sound features, to higher order speech units. We focused on intracranial recordings, as this technique allows monitoring brain activity with high spatial, temporal, and spectral resolution, and therefore is a good candidate to investigate inner speech. Despite intense efforts, investigating how the human cortex encodes inner speech remains an elusive challenge, due to the lack of behavioral and observable measures. We emphasize various challenges commonly encountered when investigating inner speech, and propose potential solutions in order to get closer to a natural speech assistive device.},
	language = {English},
	urldate = {2019-06-18},
	journal = {Frontiers in Neuroscience},
	author = {Martin, Stephanie and Iturrate, Iñaki and Millán, José del R. and Knight, Robert T. and Pasley, Brian N.},
	year = {2018},
	keywords = {inner speech, Brain-computer interface, Decoding, electrocorticography, neuroprosthetics},
	file = {Full Text PDF:/Users/Ladislas/Zotero/storage/U39N43KB/Martin et al. - 2018 - Decoding Inner Speech Using Electrocorticography .pdf:application/pdf}
}

@article{grandchamp_condialint_2019,
	title = {The {ConDialInt} {Model}: {Condensation}, {Dialogality}, and {Intentionality} {Dimensions} of {Inner} {Speech} {Within} a {Hierarchical} {Predictive} {Control} {Framework}},
	volume = {10},
	issn = {1664-1078},
	shorttitle = {The {ConDialInt} {Model}},
	url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2019.02019/full},
	doi = {10.3389/fpsyg.2019.02019},
	abstract = {Inner speech has been shown to vary in form along several dimensions. Along condensation, condensed inner speech forms have been described, that are supposed to be deprived of acoustic, phonological and even syntactic qualities. Expanded forms, on the other extreme, display articulatory and auditory properties. Along dialogality, inner speech can be monologal, when we engage in internal soliloquy, or dialogal, when we recall past conversations or imagine future dialogues involving our own voice as well as that of others addressing us. Along intentionality, it can be intentional (when we deliberately rehearse material in short-term memory) or it can arise unintentionally (during mind wandering). We introduce the ConDialInt model, a neurocognitive predictive control model of inner speech that accounts for its varieties along these three dimensions. ConDialInt spells out the condensation dimension by including inhibitory control at the conceptualization, formulation or articulatory planning stage. It accounts for dialogality, by assuming internal model adaptations and by speculating on neural processes underlying perspective switching. It explains the differences between intentional and spontaneous varieties in terms of monitoring. We present an fMRI study in which we probed varieties of inner speech along dialogality and intentionality, to examine the validity of the neuroanatomical correlates posited in ConDialInt. Condensation was also informally tackled. Our data support the hypothesis that expanded inner speech recruits speech production processes down to articulatory planning, resulting in a predicted signal, the inner voice, with auditory qualities. Along dialogality, covertly using an avatar’s voice resulted in the activation of right hemisphere homologues of the regions involved in internal own-voice soliloquy and in cerebellar deactivation, consistent with internal model adaptation. Switching from first-person to third-person perspective resulted in activations in precuneus and parietal lobules. Along intentionality, compared with intentional inner speech, mind wandering with inner speech episodes was associated with greater bilateral inferior frontal activation and decreased activation in left temporal regions. This is consistent with the reported subjective evanescence and presumably reflects condensation processes. Our results provide neuroanatomical evidence compatible with predictive control and in favor of the assumptions made in the ConDialInt model.},
	language = {English},
	urldate = {2020-03-03},
	journal = {Frontiers in Psychology},
	author = {Grandchamp, Romain and Rapin, Lucile and Perrone-Bertolotti, Marcela and Pichat, Cédric and Haldin, Célise and Cousin, Emilie and Lachaux, Jean-Philippe and Dohen, Marion and Perrier, Pascal and Garnier, Maëva and Baciu, Monica and Lœvenbruck, Hélène},
	year = {2019},
	note = {Publisher: Frontiers},
	keywords = {inner speech, mind wandering, fMRI, Auditory Verbal Imagery, condensation, dialogicality, intentionality, predictive control},
	file = {Grandchamp et al_2019_The ConDialInt Model.pdf:/Users/Ladislas/Zotero/storage/NAWYD9Q6/Grandchamp et al_2019_The ConDialInt Model.pdf:application/pdf}
}

@article{angrick_speech_2019,
	title = {Speech synthesis from {ECoG} using densely connected {3D} convolutional neural networks},
	volume = {16},
	issn = {1741-2552},
	url = {https://doi.org/10.1088%2F1741-2552%2Fab0c59},
	doi = {10.1088/1741-2552/ab0c59},
	abstract = {Objective. Direct synthesis of speech from neural signals could provide a fast and natural way of communication to people with neurological diseases. Invasively-measured brain activity (electrocorticography; ECoG) supplies the necessary temporal and spatial resolution to decode fast and complex processes such as speech production. A number of impressive advances in speech decoding using neural signals have been achieved in recent years, but the complex dynamics are still not fully understood. However, it is unlikely that simple linear models can capture the relation between neural activity and continuous spoken speech. Approach. Here we show that deep neural networks can be used to map ECoG from speech production areas onto an intermediate representation of speech (logMel spectrogram). The proposed method uses a densely connected convolutional neural network topology which is well-suited to work with the small amount of data available from each participant. Main results. In a study with six participants, we achieved correlations up to r = 0.69 between the reconstructed and original logMel spectrograms. We transfered our prediction back into an audible waveform by applying a Wavenet vocoder. The vocoder was conditioned on logMel features that harnessed a much larger, pre-existing data corpus to provide the most natural acoustic output. Significance. To the best of our knowledge, this is the first time that high-quality speech has been reconstructed from neural recordings during speech production using deep neural networks.},
	language = {en},
	number = {3},
	urldate = {2020-03-17},
	journal = {Journal of Neural Engineering},
	author = {Angrick, Miguel and Herff, Christian and Mugler, Emily and Tate, Matthew C. and Slutzky, Marc W. and Krusienski, Dean J. and Schultz, Tanja},
	month = apr,
	year = {2019},
	note = {Publisher: IOP Publishing},
	pages = {036019},
	file = {Angrick et al_2019_Speech synthesis from ECoG using densely connected 3D convolutional neural.pdf:/Users/Ladislas/Zotero/storage/F3IN7PPU/Angrick et al_2019_Speech synthesis from ECoG using densely connected 3D convolutional neural.pdf:application/pdf}
}

@article{herff_generating_2019,
	title = {Generating {Natural}, {Intelligible} {Speech} {From} {Brain} {Activity} in {Motor}, {Premotor}, and {Inferior} {Frontal} {Cortices}},
	volume = {13},
	issn = {1662-453X},
	url = {https://www.frontiersin.org/articles/10.3389/fnins.2019.01267/full},
	doi = {10.3389/fnins.2019.01267},
	abstract = {Neural interfaces that directly produce intelligible speech from brain activity would allow people with severe impairment from neurological disorders to communicate more naturally. Here, we record neural population activity in motor, premotor and inferior frontal cortices during speech production using electrocorticography (ECoG) and show that ECoG signals alone can be used to generate intelligible speech output that can preserve conversational cues. To produce speech directly from neural data, we adapted a method from the field of speech synthesis called unit selection, in which units of speech are concatenated to form audible output. In our approach, which we call {\textbackslash}emph\{Brain-To-Speech\}, we chose subsequent units of speech based on the measured ECoG activity to generate audio waveforms directly from the neural recordings. {\textbackslash}emph\{Brain-To-Speech\} employed the user's own voice to generate speech that sounded very natural and included features such as prosody and accentuation. By investigating the brain areas involved in speech production separately, we found that speech motor cortex provided more information for the reconstruction process than the other cortical areas.},
	language = {English},
	urldate = {2020-03-17},
	journal = {Frontiers in Neuroscience},
	author = {Herff, Christian and Diener, Lorenz and Angrick, Miguel and Mugler, Emily and Tate, Matthew C. and Goldrick, Matthew A. and Krusienski, Dean J. and Slutzky, Marc W. and Schultz, Tanja},
	year = {2019},
	note = {Publisher: Frontiers},
	keywords = {Speech, BCI (Brain Computer Interface), brain-to-speech, ECoG, Synthesis},
	file = {Herff et al_2019_Generating Natural, Intelligible Speech From Brain Activity in Motor, Premotor,.pdf:/Users/Ladislas/Zotero/storage/QPUK2RYT/Herff et al_2019_Generating Natural, Intelligible Speech From Brain Activity in Motor, Premotor,.pdf:application/pdf}
}

@article{makin_machine_2020,
	title = {Machine translation of cortical activity to text with an encoder–decoder framework},
	copyright = {2020 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1546-1726},
	url = {https://www.nature.com/articles/s41593-020-0608-8},
	doi = {10.1038/s41593-020-0608-8},
	abstract = {Makin and colleagues decode speech from neural signals recorded during a preoperative procedure, using an algorithm inspired by machine translation. For one participant reading from a closed set of 50 sentences, decoding accuracy is nearly perfect.},
	language = {en},
	urldate = {2020-03-31},
	journal = {Nature Neuroscience},
	author = {Makin, Joseph G. and Moses, David A. and Chang, Edward F.},
	month = mar,
	year = {2020},
	note = {Publisher: Nature Publishing Group},
	pages = {1--8},
	file = {Snapshot:/Users/Ladislas/Zotero/storage/J46N4FQB/s41593-020-0608-8.html:text/html}
}

@article{martin_use_2019,
	title = {The use of intracranial recordings to decode human language: {Challenges} and opportunities},
	volume = {193},
	issn = {1090-2155},
	shorttitle = {The use of intracranial recordings to decode human language},
	doi = {10.1016/j.bandl.2016.06.003},
	abstract = {Decoding speech from intracranial recordings serves two main purposes: understanding the neural correlates of speech processing and decoding speech features for targeting speech neuroprosthetic devices. Intracranial recordings have high spatial and temporal resolution, and thus offer a unique opportunity to investigate and decode the electrophysiological dynamics underlying speech processing. In this review article, we describe current approaches to decoding different features of speech perception and production - such as spectrotemporal, phonetic, phonotactic, semantic, and articulatory components - using intracranial recordings. A specific section is devoted to the decoding of imagined speech, and potential applications to speech prosthetic devices. We outline the challenges in decoding human language, as well as the opportunities in scientific and neuroengineering applications.},
	language = {eng},
	journal = {Brain and Language},
	author = {Martin, Stephanie and Millán, José Del R. and Knight, Robert T. and Pasley, Brian N.},
	year = {2019},
	pmid = {27377299},
	pmcid = {PMC5203979},
	keywords = {Humans, Speech, Speech Perception, Semantics, Language, Phonetics, Time course, Electrodes, Implanted, Electrocorticography, Speech decoding, Imagined speech, Intracranial recording, Neuroprosthetics, Spatio-temporal pattern of brain activity},
	pages = {73--83}
}

@article{perrone-bertolotti_what_2014,
	title = {What is that little voice inside my head? {Inner} speech phenomenology, its role in cognitive performance, and its relation to self-monitoring},
	volume = {261},
	issn = {1872-7549},
	shorttitle = {What is that little voice inside my head?},
	doi = {10.1016/j.bbr.2013.12.034},
	abstract = {The little voice inside our head, or inner speech, is a common everyday experience. It plays a central role in human consciousness at the interplay of language and thought. An impressive host of research works has been carried out on inner speech these last fifty years. Here we first describe the phenomenology of inner speech by examining five issues: common behavioural and cerebral correlates with overt speech, different types of inner speech (wilful verbal thought generation and verbal mind wandering), presence of inner speech in reading and in writing, inner signing and voice-hallucinations in deaf people. Secondly, we review the role of inner speech in cognitive performance (i.e., enhancement vs. perturbation). Finally, we consider agency in inner speech and how our inner voice is known to be self-generated and not produced by someone else.},
	language = {eng},
	journal = {Behavioural Brain Research},
	author = {Perrone-Bertolotti, M. and Rapin, L. and Lachaux, J. P. and Baciu, M. and Lœvenbruck, H.},
	month = mar,
	year = {2014},
	pmid = {24412278},
	keywords = {Consciousness, Humans, Brain, Cognition, Thinking, inner speech, Verbal Behavior, Language, auditory verbal hallucination, inner signing, inner speech monitoring, inner voice, silent reading, verbal mind wandering, verbal thoughts},
	pages = {220--239}
}

@article{nalborczyk_can_2020,
	title = {Can we decode phonetic features in inner speech using surface electromyography?},
	volume = {15},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0233282},
	doi = {10.1371/journal.pone.0233282},
	abstract = {Although having a long history of scrutiny in experimental psychology, it is still controversial whether wilful inner speech (covert speech) production is accompanied by specific activity in speech muscles. We present the results of a preregistered experiment looking at the electromyographic correlates of both overt speech and inner speech production of two phonetic classes of nonwords. An automatic classification approach was undertaken to discriminate between two articulatory features contained in nonwords uttered in both overt and covert speech. Although this approach led to reasonable accuracy rates during overt speech production, it failed to discriminate inner speech phonetic content based on surface electromyography signals. However, exploratory analyses conducted at the individual level revealed that it seemed possible to distinguish between rounded and spread nonwords covertly produced, in two participants. We discuss these results in relation to the existing literature and suggest alternative ways of testing the engagement of the speech motor system during wilful inner speech production.},
	language = {en},
	number = {5},
	urldate = {2020-06-02},
	journal = {PLOS ONE},
	author = {Nalborczyk, Ladislas and Grandchamp, Romain and Koster, Ernst H. W. and Perrone-Bertolotti, Marcela and Lœvenbruck, Hélène},
	month = may,
	year = {2020},
	note = {Publisher: Public Library of Science},
	keywords = {Electromyography, Speech, Phonology, Vowels, Acoustic signals, Speech signal processing, Face, Motor system},
	pages = {e0233282},
	file = {Nalborczyk et al_2020_Can we decode phonetic features in inner speech using surface electromyography.pdf:/Users/Ladislas/Zotero/storage/M2SARAFH/Nalborczyk et al_2020_Can we decode phonetic features in inner speech using surface electromyography.pdf:application/pdf;Snapshot:/Users/Ladislas/Zotero/storage/J4AK7CUR/article.html:text/html}
}

@article{willett_high-performance_2020,
	title = {High-performance brain-to-text communication via imagined handwriting},
	copyright = {© 2020, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
	url = {https://www.biorxiv.org/content/10.1101/2020.07.01.183384v1},
	doi = {10.1101/2020.07.01.183384},
	abstract = {{\textless}p{\textgreater}Brain-computer interfaces (BCIs) can restore communication to people who have lost the ability to move or speak. To date, a major focus of BCI research has been on restoring gross motor skills, such as reaching and grasping or point-and-click typing with a 2D computer cursor. However, rapid sequences of highly dexterous behaviors, such as handwriting or touch typing, might enable faster communication rates. Here, we demonstrate an intracortical BCI that can decode imagined handwriting movements from neural activity in motor cortex and translate it to text in real-time, using a novel recurrent neural network decoding approach. With this BCI, our study participant (whose hand was paralyzed) achieved typing speeds that exceed those of any other BCI yet reported: 90 characters per minute at \&gt;99\% accuracy with a general-purpose autocorrect. These speeds are comparable to able-bodied smartphone typing speeds in our participant9s age group (115 characters per minute) and significantly close the gap between BCI-enabled typing and able-bodied typing rates. Finally, new theoretical considerations explain why temporally complex movements, such as handwriting, may be fundamentally easier to decode than point-to-point movements. Our results open a new approach for BCIs and demonstrate the feasibility of accurately decoding rapid, dexterous movements years after paralysis.{\textless}/p{\textgreater}},
	language = {en},
	urldate = {2020-07-06},
	journal = {bioRxiv},
	author = {Willett, Francis R. and Avansino, Donald T. and Hochberg, Leigh R. and Henderson, Jaimie M. and Shenoy, Krishna V.},
	month = jul,
	year = {2020},
	note = {Publisher: Cold Spring Harbor Laboratory
Section: New Results},
	pages = {2020.07.01.183384},
	file = {Snapshot:/Users/Ladislas/Zotero/storage/GUDFQU57/2020.07.01.html:text/html;Willett et al_2020_High-performance brain-to-text communication via imagined handwriting.pdf:/Users/Ladislas/Zotero/storage/26T9HHR3/Willett et al_2020_High-performance brain-to-text communication via imagined handwriting.pdf:application/pdf}
}

@inproceedings{zhu_effects_2020,
	title = {The {Effects} of {Electrode} {Locations} on {Silent} {Speech} {Recognition} using {High}-{Density} {sEMG}},
	doi = {10.1109/MetroInd4.0IoT48571.2020.9138289},
	abstract = {In circumstances when silent speech is necessary, it is an attractive technology to use the surface electromyography (sEMG) signals recorded from electrodes placed on the face and neck regions for automatic speech recognition. For this technology, the electrode location is an important factor for the performance of speech recognition. However, it remains unclear how the electrode positions affect the performance, so there is no guideline for electrode placement in practical applications. Thus, the method of high-density sEMG was proposed to record sEMG signals from four electrode arrays over the facial and neck muscles. The high-density sEMG signals were utilized to analyze the effects of the electrode locations on the classification accuracies when increasing the category number of speaking tasks. The results showed that the increase in category numbers of speaking tasks would lead to the decline of classification accuracies, no matter which of the four electrode arrays were used. Meanwhile, the declining rates were the highest when using signals from the face electrodes (F-40ch), while those were the lowest when using the neck electrodes (NO-40ch and NE-40ch). There was no significant difference in the accuracies between the cases of the NO-40ch and NE-40ch electrodes. The findings indicated that the muscles on the neck region might be a more important contributor for automatic silent speech recognition. The study might provide new clues and guidelines for electrode placement when using sEMG for automatic silent speech recognition, which is important to develop a practical communication system for dysphonia.},
	booktitle = {2020 {IEEE} {International} {Workshop} on {Metrology} for {Industry} 4.0 {IoT}},
	author = {Zhu, Mingxing and Wang, Xiaochen and Wang, Xin and Wang, Cheng and Yang, Zijian and Williams Samuel, Oluwarotimi and Chen, Shixiong and Li, Guanglin},
	month = jun,
	year = {2020},
	keywords = {classification accuracy, facial and neck muscles, high-density surface electromyography, silent speech recognition},
	pages = {345--348},
	file = {IEEE Xplore Abstract Record:/Users/Ladislas/Zotero/storage/DRMBGP7L/9138289.html:text/html}
}

@article{schultz_biosignal-based_2017,
	title = {Biosignal-{Based} {Spoken} {Communication}: {A} {Survey}},
	volume = {25},
	issn = {2329-9304},
	shorttitle = {Biosignal-{Based} {Spoken} {Communication}},
	doi = {10.1109/TASLP.2017.2752365},
	abstract = {Speech is a complex process involving a wide range of biosignals, including but not limited to acoustics. These biosignals-stemming from the articulators, the articulator muscle activities, the neural pathways, and the brain itself-can be used to circumvent limitations of conventional speech processing in particular, and to gain insights into the process of speech production in general. Research on biosignal-based speech processing is a wide and very active field at the intersection of various disciplines, ranging from engineering, computer science, electronics and machine learning to medicine, neuroscience, physiology, and psychology. Consequently, a variety of methods and approaches have been used to investigate the common goal of creating biosignal-based speech processing devices for communication applications in everyday situations and for speech rehabilitation, as well as gaining a deeper understanding of spoken communication. This paper gives an overview of the various modalities, research approaches, and objectives for biosignal-based spoken communication.},
	number = {12},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Schultz, Tanja and Wand, Michael and Hueber, Thomas and Krusienski, Dean J. and Herff, Christian and Brumberg, Jonathan S.},
	month = dec,
	year = {2017},
	note = {Conference Name: IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	keywords = {physiology, Biology, Electromyography, Electroencephalography, electromyography, electrocorticography, electroencephalography, medical signal processing, speech production, articulator muscle activities, articulators, Biomedical measurement, Biomedical signal processing, biosignal-based speech processing, biosignal-based spoken communication, biosignals, Biosignals, communication applications, complex process, conventional speech, functional near-infrared spectroscopy, Infrared spectra, multimodal technologies, neural pathways, neurophysiology, psychology, Spectroscopy, speech processing, Speech recognition, speech recognition and synthesis, speech rehabilitation, Speech synthesis, spoken communication, Ultrasonic imaging, ultrasound, very active field, wide field},
	pages = {2257--2271},
	file = {IEEE Xplore Abstract Record:/Users/Ladislas/Zotero/storage/T9L6VRHE/8114358.html:text/html}
}

@article{hueber_statistical_2016,
	title = {Statistical conversion of silent articulation into audible speech using full-covariance {HMM}},
	volume = {36},
	issn = {0885-2308},
	url = {http://www.sciencedirect.com/science/article/pii/S0885230815000340},
	doi = {10.1016/j.csl.2015.03.005},
	abstract = {This article investigates the use of statistical mapping techniques for the conversion of articulatory movements into audible speech with no restriction on the vocabulary, in the context of a silent speech interface driven by ultrasound and video imaging. As a baseline, we first evaluated the GMM-based mapping considering dynamic features, proposed by Toda et al. (2007) for voice conversion. Then, we proposed a ‘phonetically-informed’ version of this technique, based on full-covariance HMM. This approach aims (1) at modeling explicitly the articulatory timing for each phonetic class, and (2) at exploiting linguistic knowledge to regularize the problem of silent speech conversion. Both techniques were compared on continuous speech, for two French speakers (one male, one female). For modal speech, the HMM-based technique showed a lower spectral distortion (objective evaluation). However, perceptual tests (transcription and XAB discrimination tests) showed a better intelligibility of the GMM-based technique, probably related to its less fluctuant quality. For silent speech, a perceptual identification test revealed a better segmental intelligibility for the HMM-based technique on consonants.},
	language = {en},
	urldate = {2020-07-22},
	journal = {Computer Speech \& Language},
	author = {Hueber, Thomas and Bailly, Gérard},
	month = mar,
	year = {2016},
	keywords = {Articulatory–acoustic mapping, GMM, HMM, Silent speech interface, Ultrasound},
	pages = {274--293},
	file = {ScienceDirect Snapshot:/Users/Ladislas/Zotero/storage/73GLPYFP/S0885230815000340.html:text/html}
}
